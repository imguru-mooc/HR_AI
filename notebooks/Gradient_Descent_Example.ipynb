{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imguru-mooc/HR_AI/blob/main/notebooks/Gradient_Descent_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📌 Gradient Descent (경사 하강법) 개념 정리\n",
        "\n",
        "## ✅ Gradient Descent란?\n",
        "- 함수의 최솟값을 찾는 최적화 알고리즘입니다.\n",
        "- 머신러닝에서는 모델의 비용 함수(cost function)를 최소화하는 파라미터(가중치)를 찾을 때 사용합니다.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 수식으로 표현\n",
        "\n",
        "- 경사 하강법 공식:\n",
        "\n",
        "$$\n",
        "\\theta = \\theta - \\alpha \\cdot \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
        "$$\n",
        "\n",
        "- 의미:\n",
        "  - $\\theta$: 파라미터 (예: 가중치)\n",
        "  - $\\alpha$: 학습률 (Learning Rate)\n",
        "  - $\\frac{\\partial J(\\theta)}{\\partial \\theta}$: 비용 함수 $J(\\theta)$에 대한 미분값 (그래디언트)\n",
        "\n",
        "- 동작 방식:\n",
        "  - 현재 위치에서 비용 함수의 기울기(경사)를 따라 **반대 방향으로 이동**\n",
        "  - 반복적으로 이동하며 비용 함수를 최소화\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 직관적 이해\n",
        "- 산 위에 있는 사람이 가장 낮은 지점(골짜기)로 내려가는 과정과 유사\n",
        "- 현재 위치의 경사를 계산해서 낮은 방향으로 조금씩 이동\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 하이퍼파라미터: 학습률 (Learning Rate)\n",
        "- 학습률 $\\alpha$는 한 번의 업데이트에서 이동하는 크기\n",
        "- 너무 작으면 → 학습이 느림\n",
        "- 너무 크면 → 발산하거나 최솟값을 지나쳐서 수렴 실패\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Gradient Descent의 종류\n",
        "1. **Batch Gradient Descent**\n",
        "   - 전체 데이터를 사용해서 기울기 계산\n",
        "   - 정확하지만 계산이 느림\n",
        "2. **Stochastic Gradient Descent (SGD)**\n",
        "   - 데이터 한 개씩 기울기 계산\n",
        "   - 빠르고 가볍지만 불안정\n",
        "3. **Mini-batch Gradient Descent**\n",
        "   - 일부 데이터 묶음(Batch)으로 계산\n",
        "   - 실무에서 가장 많이 사용\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 머신러닝에서의 활용\n",
        "- 선형 회귀, 로지스틱 회귀, 신경망 등 대부분의 머신러닝 모델에서 비용 함수 최적화에 사용됨\n",
        "- 경사 하강법이 잘 동작하기 위해서:\n",
        "  - 적절한 학습률 선택\n",
        "  - 비용 함수가 미분 가능해야 함\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ 시각적 요약\n",
        "- 반복적으로 비용 함수의 경사를 따라 내려가면서 최적의 파라미터를 찾는 과정\n",
        "- 비용 함수 값은 반복 횟수에 따라 점점 줄어듦\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LEcubnxgZqVy"
      },
      "id": "LEcubnxgZqVy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goxlqYe7Za7s"
      },
      "source": [
        "# Gradient Descent 실습\n",
        "\n",
        "기본적인 경사 하강법(Gradient Descent)을 이용한 선형 회귀 실습입니다."
      ],
      "id": "goxlqYe7Za7s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vuhfcd2XZa7t"
      },
      "outputs": [],
      "source": [
        "# 라이브러리 불러오기\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "Vuhfcd2XZa7t"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vip57DHHZa7u"
      },
      "outputs": [],
      "source": [
        "# 데이터 생성 (y = 4 + 3x + 노이즈)\n",
        "np.random.seed(0)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "plt.scatter(X, y)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Generated Data\")\n",
        "plt.show()"
      ],
      "id": "vip57DHHZa7u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO5MVEkwZa7u"
      },
      "outputs": [],
      "source": [
        "# 비용 함수 (Mean Squared Error)\n",
        "def compute_cost(X, y, theta):\n",
        "    m = len(y)\n",
        "    predictions = X.dot(theta)\n",
        "    cost = (1/2*m) * np.sum(np.square(predictions - y))\n",
        "    return cost"
      ],
      "id": "lO5MVEkwZa7u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzWiBgFTZa7u"
      },
      "outputs": [],
      "source": [
        "# 경사 하강법 함수\n",
        "def gradient_descent(X, y, theta, learning_rate, iterations):\n",
        "    m = len(y)\n",
        "    cost_history = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        prediction = np.dot(X, theta)\n",
        "        error = prediction - y\n",
        "        gradients = (1/m) * X.T.dot(error)\n",
        "        theta = theta - learning_rate * gradients\n",
        "        cost = compute_cost(X, y, theta)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return theta, cost_history"
      ],
      "id": "UzWiBgFTZa7u"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qtAuoo3Za7v"
      },
      "outputs": [],
      "source": [
        "# 데이터 준비 (bias term 추가)\n",
        "X_b = np.c_[np.ones((100, 1)), X]\n",
        "theta = np.random.randn(2, 1)  # 초기 가중치 (랜덤)"
      ],
      "id": "0qtAuoo3Za7v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vze5rgmZa7v"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터 설정 및 Gradient Descent 실행\n",
        "learning_rate = 0.1\n",
        "iterations = 100\n",
        "\n",
        "theta_final, cost_history = gradient_descent(X_b, y, theta, learning_rate, iterations)\n",
        "\n",
        "print(\"최종 파라미터 (theta):\")\n",
        "print(theta_final)"
      ],
      "id": "5vze5rgmZa7v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaQm4aHEZa7v"
      },
      "outputs": [],
      "source": [
        "# 비용 함수 시각화\n",
        "plt.plot(range(iterations), cost_history, 'b-')\n",
        "plt.xlabel(\"Iterations\")\n",
        "plt.ylabel(\"Cost (MSE)\")\n",
        "plt.title(\"Cost Reduction Over Iterations\")\n",
        "plt.show()"
      ],
      "id": "YaQm4aHEZa7v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3UoB6cfZa7v"
      },
      "outputs": [],
      "source": [
        "# 회귀선 시각화\n",
        "plt.scatter(X, y)\n",
        "plt.plot(X, X_b.dot(theta_final), color='red')\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.title(\"Regression Line after Gradient Descent\")\n",
        "plt.show()"
      ],
      "id": "w3UoB6cfZa7v"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MSE 손실 함수 그래프"
      ],
      "metadata": {
        "id": "fZXWB_fsafZC"
      },
      "id": "fZXWB_fsafZC"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np   # numeriacal computing\n",
        "import matplotlib.pyplot as plt  # plotting core\n",
        "\n",
        "w = np.linspace(-1,3,100)\n",
        "b = 0\n",
        "x = 2\n",
        "y = 2\n",
        "\n",
        "j = np.zeros(100)\n",
        "\n",
        "for i in range(len(w)):\n",
        "    y_hat = w[i]*x + b;\n",
        "    j[i] = 0.5 * (y_hat - y)**2\n",
        "\n",
        "plt.plot(w,j, 'o' )\n",
        "plt.plot(w,j, 'r-' )\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vYQGXu5-ahir"
      },
      "id": "vYQGXu5-ahir",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 경사 하강법 시각화"
      ],
      "metadata": {
        "id": "U2XtO7gPap21"
      },
      "id": "U2XtO7gPap21"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np   # numeriacal computing\n",
        "import matplotlib.pyplot as plt  # plotting core\n",
        "\n",
        "# simple function to demo step size\n",
        "def f(x) : # A parabola\n",
        "    f = 0.5*(2-x*2)**2\n",
        "    return f\n",
        "def Df(x) : # The derivative (gradient)\n",
        "    Df = 2*(2-x*2)\n",
        "    return Df\n",
        "def xp1(x,alpha) : # update\n",
        "    xp1 = x + alpha * Df(x)\n",
        "    return xp1\n",
        "\n",
        "def plot_steps( guess, alpha, nsteps) :\n",
        "    fig, ax = plt.subplots()\n",
        "    x = np.linspace(-1,3,100)\n",
        "    ax.plot(x, f(x))\n",
        "    x = guess\n",
        "    ax.plot(x,f(x), 'o', label='start x=%.2f' %x )\n",
        "    for i in range(nsteps):\n",
        "        xold = x\n",
        "        x = xp1(x,alpha)\n",
        "        #ax.plot(x,f(x), 'o', label='x = %.2f' %x)\n",
        "        ax.plot(x,f(x), 'o')\n",
        "        ax.plot([xold,x],[f(xold),f(x)], '-')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_steps(3, 0.01, 50)"
      ],
      "metadata": {
        "id": "_afmqH0aakgb"
      },
      "id": "_afmqH0aakgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습 과정 실제 그래프 (weight, bias 둘다 학습)"
      ],
      "metadata": {
        "id": "DbgfrxcKa2KN"
      },
      "id": "DbgfrxcKa2KN"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(1,1,1,projection='3d')\n",
        "\n",
        "def pprint(arr):\n",
        "    print(\"type:{}\".format(type(arr)))\n",
        "    print(\"shape: {}, dimension: {}, dtype:{}\".format(arr.shape, arr.ndim, arr.dtype))\n",
        "    print(\"Array's Data:\\n\", arr)\n",
        "\n",
        "# The function J\n",
        "def J(a0, a1, x, y, m):\n",
        "    ret=0\n",
        "    for i in range(m):\n",
        "        ret += 0.5*((a0 + a1*x[i]) - y[i] )**2\n",
        "    return ret/m\n",
        "\n",
        "x = np.linspace(-1,1,5)\n",
        "y = x\n",
        "\n",
        "a0 = np.linspace(-3,3,100)\n",
        "a1 = np.linspace(-3,3,100)\n",
        "\n",
        "aa0, aa1 = np.meshgrid(a0, a1)\n",
        "ax.plot_surface(aa0, aa1, J(aa0,aa1,x,y,m=len(x)))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vM2jFGJTas73"
      },
      "id": "vM2jFGJTas73",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실전 데이터를 이용한 선형회귀"
      ],
      "metadata": {
        "id": "6TCX_1LJbqdQ"
      },
      "id": "6TCX_1LJbqdQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "diabetes = load_diabetes()\n",
        "import matplotlib.pyplot as plt\n",
        "x = diabetes.data[:, 2]\n",
        "y = diabetes.target\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2mGHYJG6a_l3"
      },
      "id": "2mGHYJG6a_l3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = 1.0\n",
        "b = 1.0\n",
        "for x_i, y_i in zip(x, y):\n",
        "    y_hat = x_i * w + b\n",
        "    err = y_hat - y_i\n",
        "    rate = 0.01\n",
        "    w = w - rate * err * x_i\n",
        "    b = b - rate * err\n",
        "\n",
        "plt.scatter(x, y)\n",
        "pt1 = (-0.1, -0.1 * w + b)\n",
        "pt2 = (0.15, 0.15 * w + b)\n",
        "plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]], 'r-')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W20mG0Ipbxie"
      },
      "id": "W20mG0Ipbxie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = 1.0\n",
        "b = 1.0\n",
        "for i in range(1, 1000):\n",
        "    for x_i, y_i in zip(x, y):\n",
        "        y_hat = x_i * w + b\n",
        "        err = y_hat - y_i\n",
        "        rate = 0.01\n",
        "        w = w - rate * err * x_i\n",
        "        b = b - rate * err\n",
        "plt.scatter(x, y)\n",
        "print(w)\n",
        "print(b)\n",
        "pt1 = (-0.1, -0.1 * w + b)\n",
        "pt2 = (0.15, 0.15 * w + b)\n",
        "plt.plot([pt1[0], pt2[0]], [pt1[1], pt2[1]], 'r-')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uCRfA0D6b5eT"
      },
      "id": "uCRfA0D6b5eT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 학습된 파라메터를 이용한 값 예측"
      ],
      "metadata": {
        "id": "7hpXeqgScASe"
      },
      "id": "7hpXeqgScASe"
    },
    {
      "cell_type": "code",
      "source": [
        "x_new = 0.18\n",
        "y_pred = x_new * w + b\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "AV9Nrpz5b9a8"
      },
      "id": "AV9Nrpz5b9a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x, y)\n",
        "plt.scatter(x_new, y_pred)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hKEFRnzgcEyP"
      },
      "id": "hKEFRnzgcEyP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 머신러닝  vs  딥러닝  시연"
      ],
      "metadata": {
        "id": "rFs_Ip6gcLYn"
      },
      "id": "rFs_Ip6gcLYn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[playground.tensorflow.org](http://playground.tensorflow.org)"
      ],
      "metadata": {
        "id": "8WkVyXELcRaE"
      },
      "id": "8WkVyXELcRaE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 합성공 신경망(CNN)  시연\n"
      ],
      "metadata": {
        "id": "MW-cV7aIcmEw"
      },
      "id": "MW-cV7aIcmEw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "[ConvNetJS CIFAR-10 demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)"
      ],
      "metadata": {
        "id": "HCcwA_gOcstM"
      },
      "id": "HCcwA_gOcstM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 트랜스 포머 모델(Transformer) 시연"
      ],
      "metadata": {
        "id": "laUXRc8Jd7pq"
      },
      "id": "laUXRc8Jd7pq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.namu.wiki/i/VbGh4H2AXMcaQ0Nr8_PxYKyJa8X_FHrQo2jJuITdAWPuJl1QMQpKLhSgxArftKvWDsItPkJ4ewjiNfB-_s2A_TlTDUZOj2GGN3XRXnH8cUybraxqqkhZCi6vGepqbaLpv9HXG_ecdkkMBHimdcGYAA.webp\" alt=\"설명\" width=\"400\"/>\n",
        "\n",
        "[Transformer 시연](https://poloclub.github.io/transformer-explainer/)"
      ],
      "metadata": {
        "id": "pmKzG99NeDow"
      },
      "id": "pmKzG99NeDow"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT3 시연\n",
        "\n",
        "[GPT3 시연](https://bbycroft.net/llm)"
      ],
      "metadata": {
        "id": "xF_C-INDgNgS"
      },
      "id": "xF_C-INDgNgS"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3hCoZVzcHJe"
      },
      "id": "u3hCoZVzcHJe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}